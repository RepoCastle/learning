# Downloading Spark and Getting Started



## Downloading Spark

You **don't need** to have Hadoop, but if you have an existing Hadoop cluster or HDFS installation, download **the matching version** of Spark.



## Introduction to Spark's Python and Scala Shells

Spark's shells allow you to interact with data that is distributed on disk or in memory **across many machines**, and Spark takes care of automatically distributing this processing.

You can control the verbosity of the logging by creating a file in the `conf` directory called ***log4j.properties***: `log4j.rootCategory=WARN, console`



## Introduction to Core Spark Concepts

**Driver Program** and **Executors**

 - Driver program launches various parallel operations on a cluster.
 - Driver programs access Spark through a **SparkContext** object, which represents a connection to a computing cluster.
 - Driver programs typically manage a number of nodes called **executors**.



## Standalone Applications

Spark can be linked into standalone applications, and the main difference from using it in the shell is that we need to initialize your own **SparkContext** [^sparkcontext].

[^sparkcontext]: From Spark 2.0, `SparkSession` is introduced as a new entry point that subsumes `SQLContext` and `HiveContext`.

In order to use Spark in standalone applications, We have to give the applications **a dependency on the `spark-core` artifact**.

### Initializing a SparkContext

 1. import the Spark packages
 2. create a `SparkConf` object to configure the application. [(detail)](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf)
 3. builde a `SparkContext` for it

``` scala
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._

val conf = new SparkConf().setMaster("local").setAppName("My App")
val sc = new SparkContext(conf)
```

`SparkConf` can be set by [`set` API](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkConf)

To shut down Spark, you can either call the `stop()` method on the `SparkContext`, or simply exit the application.

### Building Standalone Applications

We need a `build.sbt` to build the spark applications.
A minimized sbt build file would be as follows:

``` scala
name := "learning-spark-mini-example"
version := "0.0.1"
scalaVersion := "2.10.4"

// additional libraries
libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "1.2.0" % "provided"
)
```

The dependency on spark core is marked as **provided** so that when we use an assembly JAR ***we don't include the `spark-core` JAR***, which is already on the classpath of the workers.

After building the package, we can `spark-submit` the application with `--class` to specify the main class.

