# Loading and Saving Your Data

## Motivation

Spark can access data through the `InputFormat` and `OutputFormat` interfaces used by Hadoop MapReduce.

Three common sets of data surces:
 - File formats and filesystems
 - Structured data sources through Spark SQL
 - Databases and key/value stores


## File Formats

 Text files, JSON, CSV, SequenceFiles, Protocol buffers, object files

 Hadoop's new and old file APIs for keyed (or paired) data.

### Text Files
 - **`textFile()`**: each input line becomes an element in the RDD.
 - **`wholeTextFiles()`**: the key being the name and the value being the contents of each file.

 > Spark supports reading all the files in a given directory and doing wildcard expansion on the input.

 - **`saveAsTextFile(dir)`**: output the contents of the RDD to multiple files in `dir` directory. We **don't get control which files end up with which segments** of our data.

### JSON

 Three common loading json methods:
 - Simplest way: Load json data as a text file and then mapping over the values with a JSON parser.
 - A custom Hadoop input format: `sc.newAPIHadoopFile()`
 - Sparl SQL: `spark.read.json()`

 > If you do choose to skip incorrectly formatted data, you may wish to use accumulators to keep track of the errors.



### Comma-Separated Values and Tab-Separated Values

 Similar to JSON

### SequenceFiles

### Object Files

### Hadoop Input and Output Formats

### File Compression



## Filesystems

### Local / "Regular" FS

### Amazon S3

### HDFS



## Structured Data with Spark SQL

### Apache Hive

### JSON



## Databases

### Java Database Connectivity

### Cassandra

### HBase

### Elasticsearch

