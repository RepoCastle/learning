# Loading and Saving Your Data

## Motivation

Spark can access data through the `InputFormat` and `OutputFormat` interfaces used by Hadoop MapReduce.

Three common sets of data surces:
 - File formats and filesystems
 - Structured data sources through Spark SQL
 - Databases and key/value stores


## File Formats

 Text files, JSON, CSV, SequenceFiles, Protocol buffers, object files

 Hadoop's new and old file APIs for keyed (or paired) data.

### Text Files
 - **`textFile()`**: each input line becomes an element in the RDD.
 - **`wholeTextFiles()`**: the key being the name and the value being the contents of each file.

 > Spark supports reading all the files in a given directory and doing wildcard expansion on the input.

 - **`saveAsTextFile(dir)`**: output the contents of the RDD to multiple files in `dir` directory. We **don't get control which files end up with which segments** of our data.

### JSON

 Three common loading json methods:
 - Simplest way: Load json data as a text file and then mapping over the values with a JSON parser.
 - A custom Hadoop input format: `sc.newAPIHadoopFile()`
 - Sparl SQL: `spark.read.json()`

 > If you do choose to skip incorrectly formatted data, you may wish to use accumulators to keep track of the errors.



### Comma-Separated Values and Tab-Separated Values

 Similar to JSON

### SequenceFiles

### Object Files

### Hadoop Input and Output Formats

### File Compression



## Filesystems

### Local / "Regular" FS

### Amazon S3

### HDFS



## Structured Data with Spark SQL

 By structured data, we mean **data that has schema**, that is a consistent set of fields across data records.

 As Spark understands their schema, it can efficiently **read only the fields required** from the data sources.

 **Row** object, one per record, allows access based on the column number.

### Apache Hive

 Need a `hive-site.xml` file in Spark's `./conf/` directory.

 **`HiveContext`**: `hiveCtx = HiveContext(sc)` and then `hiveCtx.sql("SELECT ... FROM ...")`

### JSON

 Spark SQL can **infer** json file's **schema** and **load data as rows**.

 **one record per line**. **nested values** are allowed.

 Through **`HiveContext`**, but Hive system is not necessary.



## Databases

 Using either **Hadoop connector** or **custom Spark connectors**.

### Java Database Connectivity

 Spark can load data from **any relational database that supports Java Database Connectivity (JDBC)**, including MySQL, Postgres, and other systems.

 - provide a function to establish a connection to the database. (This lets **each node** create **its own connection**)
 - provide a query that can read a range of the data. (`lowerBound` and `upperBound` are used to limit per node loading range)
 - converters for each row of output from a `java.sql.ResultSet`

 > Make sure that your database can handle the load of parallel reads from Spark.

### Cassandra

 Use **Spark Cassandra connector** from DataStax. **CassandraRow** is similar to **Row** object.

 - `spark.cassandra.connection.host`
 - `spark.cassandra.auth.username` and `spark.cassandra.auth.password`

 We can **restrict the data to be loaded** by adding a **`where`** clause to the `cassandraTable()` call.


### HBase

 `org.apache.hadoop.hbase.mapreduce.TableInputFormat` => `[(org.apache.hadoop.hbase.io.ImmutableBytesWritable, org.apache.hadoop.hbase.client.Result)]`



### Elasticsearch

 Elasticsearch is a new open source, Lucene-based search system.

 Elasticsearch depends on setting up configuration on our SparkContext.
