# Programming with RDDs

 > In Spark, all work is expressed as **either** *creating* new RDDs, *transforming* existing RDDs, or *calling operations* on RDDs to compute a result.





## RDD Basics

 Each RDD is split into multiple ***partitions***, which may be computed on different nodes of the cluster.

 Users create RDDs in **two** ways:
  1. by loading an external dataset.
  2. by distributing a collection of objects(e.g., a list or set) in their driver program.

 RDDs offer two types of operations: ***transformation***s and ***action***s.
  - **Transformation**: **construct a new RDD** from a previous one.
  - **Action**: **compute** a result based on an RDD, and **either** ***return it to the driver program*** or ***save it to an external storage system***.

 RDD is computed in a ***lazy*** fashion, that is, the ***first time*** they are ***used in an action***.

 > RDDs are *by default* **recomputed** ***each time you run an action on them***. Or you can use `persist` to reuse RDDs in multiple actions.





## Creating RDDs

 - `sc.textFile(...)` or others
 - `sc.parallelize(...)`





## RDD Operations

 Transformations return RDDs, whereas actions return some other data type.

### Transformations

 Many transformations are ***element-wise***, that is, they work on one element at a time; but this is not true for all transformations.

 Spark keeps track of the set of dependencies between different RDDs, called the ***lineage graph***.

 ***Lineage graph*** is used to **compute each RDD on demand** and to **recover lost data** if part of a persistent RDD is lost.

### Actions

 Actions force the evaluation of the transformations required for the RDD.

 The entire dataset of RDD with `collect()` action **must fit in memory on a single machine**, so `collect()` **shouldn't** be used on ***large datasets***.

### Lazy Evaluation

 As transformations are lazy evaluated, rather than thinking of an RDD as containing specific data, it is best to **think of each RDD as consisting of instructions on how to compute the data** that we build up through transformations.

 In Spark, there is no substantial benefit to write a single complex map instead of chaning together many simple operations.





## Passing Functions to Spark

 One issue to watch out for when passing functions is inadvertently serializing the object containing the function.

 When you pass a function that is the member of an object, or contains references to fields in an object,
 Spark sends the ***entire object*** to worker nodes, which can be much larger than the bit of information you need.





## Common Transformations and Actions

### Basic RDDs

#### Element-wise transformations

 map, filter, flatMap

#### Pseudo set operations

 - **distinct**: it requires shuffling all the data over the network
 - **union**: if there are duplicates in the input RDDs, the result of Spark's `union()` will contain duplicates.
 - **intersection**: returns only elements in both RDDs, and also removes all duplicates. (shuffle required)
 - **subtract**: shuffle required.
 - **cartesian**: return all possible pairs. (***Very very expensive*** for large RDDs)

### Converting Between RDD Types





## Persistence
