# Working with Key/Value Pairs

 - How to work with RDDs of key/value pairs
 - Let users control the layout of pair RDDs across nodes: **partitioning**

 > Using controllable partitioning, applications can sometimes greatly reduce communication costs by ensuring that data will be accessed together and will be on the same node.



## Motivation

 Pair RDDs expose operations that allow you to **act on each key in parallel** or **regroup data** across the network.

 It is common to **extract fields from an RDD** (representing, for instance, an event time, customer ID, or other identifier) and **use those fields as keys** in pair RDD operations.



## Creating Pair RDDs

 We can turn a regular RDD into a pair RDD **by running a `map()` function** that returns key/value pairs.

 When creating a pair RDD from an in-memory collection in Scala, we only need to call `SparkContext.parallelize()` on a collection of pairs.



## Transformations on Pair RDDs

 Pair RDDs are allowed to use **all the transformations available to standard RDDs**.

 - one pair RDD: `reduceByKey(func)`, `combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner)`, `mapValues(func)`, `flatMapValues(func)`, `keys()`, `values()`, `sortByKey()`, `groupByKey()`
 - two pair RDDs: `subtractByKey`, `join`, `righOuterJoin`, `leftOuterJoin`, `cogroup`

### Aggregations
 - **`reduceByKey()`**: runs several parallel reduce operations, one for each key, where each operation combines values that have the same key. It **returns a new pair RDD**.
 - **`foldByKey()`**
 - **`mapValues()`**
 - **`combineByKey()`**: allows the user to return values that are *not the same type as our input data*. **Most of the other per-key combiners** are **implemented using it**.
   - As `combineByKey()` goes through the elements in a partition, each element either
     - has a key it hasn't seen before
     - or has the same key as a previous element.
   - Each partition is processed independently
   - If it's a the **first time found element on a partition** (***not*** the first time in the RDD), `createCombiner` is used to create the initial value for the accumulator on that key.
   - If it is a value had been processed in that partition, it will use the `mergeValue` with the current value for the accumulator for that key and the new value.
   - When merging the results from each partition, if two or more partitions have an accumulator for that same key, we use `mergeCombiners` to merge the accumulators.

 > `reduceByKey()` and `foldByKey()` will automatically perform combining locally on each machine before computing global totals for each key.

 > We can disable map-side aggregation in `combineByKey()` if we know that our data won't benefit from it. If we want to disable map-side combines, we need to specify the partitioner.

#### Tuning the level of parallelism

 Every RDD has a fixed number of *partitions* that determine the degree of parallelism to use when executing operations on the RDD.

 When performing aggregations or grouping operations, we can ask Spark to use a specific number of partitions.

 Most of the operations discussed in this chapter accept a second parameter giving the number of partitions to use when creating the grouped or aggregated RDD.

 Spark also provides the `repartition()` function, which shuffles the data across the network to create a new set of partitions. (However, **very expensive shuffle**)

 **`coalesce()`** is an optimized version of `repartition()` which allows avoiding data movement, but only if you are decreasing the number of RDD partitions.

### Grouping Data

 > With keyed data a common use case is grouping our data by key.

 **`groupByKey()`**: RDD[(K, V)] => RDD[(K, Iterable[V])]
 **`groupBy()`**: works on unpaired data or data where we want to use a different condition besides equality on the current key.

 > If you find yourself writing code where you **`groupByKey()` and then use a `reduce()` or `fold()` on the values**, you can probably achieve the same result *more efficiently* **by using one of the per-key aggregation functions**.  `rdd.reduceByKey(func) == rdd.groupByKey().mapValues(value => value.reduce(func))`

 **`cogroup()`** over two RDDs sharing the same key type, `K`, with the respective value types `V` and `W` gives us back RDD[(K, (Iterable[V], Iterable[W]))]

### Joins

### Sorting Data



## Actions Available on Pair RDDs




## Data Partitioning (Advanced)

### Determining an RDD's Partitioner

### Operations That Benefit from Partitioning

### Operations That Affect Partitioning

### Example: PageRank

### Custom Partitioners



