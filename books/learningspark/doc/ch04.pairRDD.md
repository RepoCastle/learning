# Working with Key/Value Pairs

 - How to work with RDDs of key/value pairs
 - Let users control the layout of pair RDDs across nodes: **partitioning**

 > Using controllable partitioning, applications can sometimes greatly reduce communication costs by ensuring that data will be accessed together and will be on the same node.



## Motivation

 Pair RDDs expose operations that allow you to **act on each key in parallel** or **regroup data** across the network.

 It is common to **extract fields from an RDD** (representing, for instance, an event time, customer ID, or other identifier) and **use those fields as keys** in pair RDD operations.



## Creating Pair RDDs

 We can turn a regular RDD into a pair RDD **by running a `map()` function** that returns key/value pairs.

 When creating a pair RDD from an in-memory collection in Scala, we only need to call `SparkContext.parallelize()` on a collection of pairs.



## Transformations on Pair RDDs

 Pair RDDs are allowed to use **all the transformations available to standard RDDs**.

 - one pair RDD: `reduceByKey(func)`, `combineByKey(createCombiner, mergeValue, mergeCombiners, partitioner)`, `mapValues(func)`, `flatMapValues(func)`, `keys()`, `values()`, `sortByKey()`, `groupByKey()`
 - two pair RDDs: `subtractByKey`, `join`, `righOuterJoin`, `leftOuterJoin`, `cogroup`

### Aggregations
 - **`reduceByKey()`**: runs several parallel reduce operations, one for each key, where each operation combines values that have the same key. It **returns a new pair RDD**.
 - **`foldByKey()`**
 - **`mapValues()`**
 - **`combineByKey()`**: allows the user to return values that are *not the same type as our input data*. **Most of the other per-key combiners** are **implemented using it**.
   - As `combineByKey()` goes through the elements in a partition, each element either
     - has a key it hasn't seen before
     - or has the same key as a previous element.
   - Each partition is processed independently
   - If it's a the **first time found element on a partition** (***not*** the first time in the RDD), `createCombiner` is used to create the initial value for the accumulator on that key.
   - If it is a value had been processed in that partition, it will use the `mergeValue` with the current value for the accumulator for that key and the new value.
   - When merging the results from each partition, if two or more partitions have an accumulator for that same key, we use `mergeCombiners` to merge the accumulators.

 > `reduceByKey()` and `foldByKey()` will automatically perform combining locally on each machine before computing global totals for each key.

 > We can disable map-side aggregation in `combineByKey()` if we know that our data won't benefit from it. If we want to disable map-side combines, we need to specify the partitioner.

#### Tuning the level of parallelism

 Every RDD has a fixed number of *partitions* that determine the degree of parallelism to use when executing operations on the RDD.

 When performing aggregations or grouping operations, we can ask Spark to use a specific number of partitions.

 Most of the operations discussed in this chapter accept a second parameter giving the number of partitions to use when creating the grouped or aggregated RDD.

 Spark also provides the `repartition()` function, which shuffles the data across the network to create a new set of partitions. (However, **very expensive shuffle**)

 **`coalesce()`** is an optimized version of `repartition()` which allows avoiding data movement, but only if you are decreasing the number of RDD partitions.

### Grouping Data

 > With keyed data a common use case is grouping our data by key.

 **`groupByKey()`**: RDD[(K, V)] => RDD[(K, Iterable[V])]
 **`groupBy()`**: works on unpaired data or data where we want to use a different condition besides equality on the current key.

 > If you find yourself writing code where you **`groupByKey()` and then use a `reduce()` or `fold()` on the values**, you can probably achieve the same result *more efficiently* **by using one of the per-key aggregation functions**.  `rdd.reduceByKey(func) == rdd.groupByKey().mapValues(value => value.reduce(func))`

 **`cogroup()`** over two RDDs sharing the same key type, `K`, with the respective value types `V` and `W` gives us back **`RDD[(K, (Iterable[V], Iterable[W]))]`**

### Joins

 Joining data together is probably **one of the most common operations** on a pair RDD.
 - **inner joins - `join(other)`**: only keys that are present in both pair RDDs are output. **`RDD[(K, (V, W))]`**
 - **`leftOuterJoin(other)`**: has entries for each key in the source RDD. **`RDD[(K, (V, Option[W]))]`**
 - **`rightOuterJoin(other)`**: has entries for each key in the other RDD. **`RDD[(K, (Option[V], W))]`**
 - cross joins

### Sorting Data

 **`sortByKey(ascending)`**: shuffle required

 or with **custom sort order**:

 ``` scala
  val input: RDD[(K, V)] = ...
  implicit val sortIntegersByString = new Ordering[Int] {
    override def compare(a: Int, b: Int) = a.toString.compare(b.toString)
  }
  rdd.sortByKey()
 ```


## Actions Available on Pair RDDs

 **All of the traditional actions available** on the base RDD are also available on pair RDDs.

 **`countByKey()`**, **`collectAsMap()`**, **`lookup(key)`**



## Data Partitioning (Advanced)

 Communication is very expensive, so **laying out data to minimize network traffic** can greatly improve performance.
 
 **Partitioning** is **useful only when a dataset is resued multiple times** in key-oriented operations such as joins.

 Spark's partitioning is **available on all RDDs of key/value pairs**, and causes the system to **group elements based on a function of each key**.
 It lets the program ensure that **a set of keys** will **appear together on some node**.

 - Hash partition
 - Range partition

 If `join()` operation on two pair rdds does not know anything about how the keys are partitioned in the datasets,
 it will hash all the keys of **both** datasets, sending elements with the same key hash across the network to the same machine,
 and then join together the elments with the same key on that machine.

 **`val newRDD = rdd.partitionBy()`** transformation makes rdd be partitioned, and Spark knows that the `newRDD` is partitioned,
 and calls to `newRDD.join(other)` on it will **take advantage of this partition information**,
 and Spark will **shuffle only the `other` rdd**. (**Less data communication over the network**)

 When **`partitionBy()`** is called on an RDD, the new RDD should be **`persist()`** for a better performance.

 Transformations result in an RDD with known partitioning information:
 - **`sortByKey()`**: Range partitioned
 - **`groupByKey()`**: Hash partitioned

 **`map()`** will cause the new RDD to **forget** the parent's partitioning information.

### Determining an RDD's Partitioner

 **`val partitioner: Option[spark.Partitioner] = pairrdd.partitioner`**

### Operations That Benefit from Partitioning

 All of the operations **involve shuffling data by key across the network** will benefit from partitioning.

 **`cogroup()`**, **`groupWith()`**, **`join()`**, **`leftOuterJoin()`**, **`rightOuterJoin()`**, **`groupByKey()`**, **`reduceByKey()`**, **`combineByKey()`**, and **`lookup()`**.

 On a prepartitioned RDD:
 - **operations that act on a single RDD**: such as `reduceByKey()`, all the values for each key to be computed locally on a single machine,
   requiring only the final, locally reduced value to be sent to the master.
 - **binary operations**: such as `cogroup()` and `join()`, cause at least one of the RDDs (the one with the known partitioner) to not be shuffled.
   - if both RDDs have the **same** partitioner, then **no shuffling** across the network will occur.

### Operations That Affect Partitioning

 Operations that result in a **partitioner being set**:
 - **`cogroup()`**, **`groupWith()`**, **`join()`**, **`leftOuterJoin()`**, **`rightOuterJoin()`**, **`groupByKey()`**, **`reduceByKey()`**, **`combineByKey()`**, **`partitionBy()`**, **`sort()`**
 - if parent RDD has a partitioner: **`mapValues()`**, **`flatMapValues()`**, **`filter()`**

 All other operations will produce a result with no partitioner.

 For **binary operations**, output rdd partitioner depends on the parent RDD's partitioners.
 - **Default**: **hash partitioner** (number: level of parallelism of the operation)
 - **One of the parents** has a partitioner set, it will be **that partitioner**
 - **Both parents** have a partitioner set, **the partitioner of the first parent**

### Example: PageRank

 > Use `mapValues()` or `flatMapValues()` whenever you are not changing an element's key.

### Custom Partitioners

 **`HashPartitioner`** and **`RangePartitioner`**

 To implement a custom partitioner, you need to subclass the `org.apache.spark.Partitioner` class and implement three methods:
 - **`numPartitions: Int`**
 - **`getPartion(key: Any): Int`**: returns the partition ID for a given key (`0` to `numPartitions - 1`)
 - **`equals()`**: Spark will need to test the Partitioner object against other instances when it decides wheather two of the RDDs are partitioned the same way 

