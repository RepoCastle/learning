# Spark Streaming

**DStreams** or **Discretized Streams** is a sequence of data arriving over time.

**Each DStream** is represented as **a sequence of RDDs** arriving at each time step, where **each RDD has one time slice** of the data in the stream.

 - **transformations**: which yield a new DStream
   - **stateless**
   - **stateful** transformations that can aggregate data across time
 - **output operations**: which write data to an external system: runs **periodically** on each time step.

## A Simple Example

 Example:

 > We will receive a stream of newline-delimited lines of text from a server running at port 7777, filter only the lines that contain the word *error*, and print them.

 ``` scala
 val ssc = new StreamingContext(sc, Seconds(1))
 val lines = ssc.socketTextStream("localhost", 7777)
 val errorLines = lines.filter(_.contains("error"))
 errorLines.print()
 ssc.start()
 ssc.awaitTermination()
 ```


## Architecture and Abstraction

 Spark Streaming computation is treated as **a continuous series of batch computations** on small batches of data.

 New batches are created **at regular time intervals**.

 **Each input batch forms an RDD**.

 For each input source, Spark Streaming **launches** ***receivers***,
 which are tasks **running within the application's executors**
 that **collect data** from the input source and **save it as RDDs**.

 - replicate to another executor for fault tolerance
 - stored in the memory of the executors
 - include a mechanism called **checkpointing** to save state periodically to a reliable filesystem


## Transformations

### Stateless Transformations

 > Processing of each batch does **not depend** on the data of its **previous batches**.

 **Applied on every batch** ***separately*** -- that is, every RDD in a DStream.

 Can **combine data from multiple DStreams**, again **within each time step**.

 DStreams provide an advanced operator called **transform()** that lets you **operate** directly on the RDDs inside them.
 - A common application of **`transform()`** is to **reuse batch processing code you had written on RDDs**.
 - **`StreamingContext.transform`** or **`DStream.transformWith(other, func)`**


### Stateful Transformations

 > Use data or intermediate results **from previous batches to compute the results** of the current batch.

 - **windowed operations**: act over a sliding window of time periods
 - **`updateStateByKey()`**: used to track state across events for each key

 Require **checkpointing** to be enabled in the StreamingContext for fault tolerance.

 All windowed operations need two parameters, **window duration** and **sliding duration**, both of which must be **a multiple of the StreamingContext's batch interval**.
 - **`windowDuration`**: controls how many previous batches of data are considered.
 - **sliding duration**: controls how frequently the new DStream computes results.

 **`dStream.window(Seconds(30), Seconds(10))`**


 - **`window()`**
 - **`reduceByWindow()`**, **`reduceByKeyAndWindow()`**
 - **`countByWindow()`**, **`countByValueAndWindow()`**
 - **`updateStateByKey(updateFunc: (Seq[V], Option[S]) => Option[S])`** transformation


## Output Operations

 - **`print()`**
 - **`save()`**
 - **`saveAsHadoopFiles()`**
 - **`foreachRDD()`**



## Input Sources

### Core Sources

### Additional Sources

### Multiple Sources and Cluster Sizing



## 24/7 Operation

### Checkpointing

### Driver Fault Tolerance

### Worker Fault Tolerance

### Receiver Fault Tolerance

### Processing Guarantees



## Streaming UI




## Performance Considerations

### Batch and Window Sizes

### Level of Parallelism

### Garbage Collection and Memory Usage


