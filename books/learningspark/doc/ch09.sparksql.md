# Spark SQL

 > As Spark SQL has been improved a lot in 2.1, lots of differences exist in the latest Spark SQL and the version used in the book, so the main content is based on the [official document](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html).

## Overview

 Spark SQL uses the extra **structure of the data and the computation being performed information** to perform extra **optimizations**.

 There are several ways to interact with Spark SQL including **SQL** and **the Dataset API**.

### SQL

 - The results of running SQL **within another programming language** will be returned as a **`Dataset/DataFrame`**.
 - **Interact with the SQL** interface using the **commandline** or **over JDBC/ODBC**.

### Datasets and DataFrames

 A **Dataset&** is **a distributed collection** of data.
 - benefits of RDDs (**strong typing**, ability to use **powerful lambda functions**)
 - benefits of Spark SQL's **optimized execution engine**.

 A **DataFrame** is a **`Dataset[Row]`** **organized into named columns** which is **conceptually equivalent to a table** in a relational database or a data frame in R/Python.
 - **Sources**: structured data files, tables in Hive, external databases, or existing RDDs.


## Getting Started

### Starting Point: SparkSession

 ``` scala
 val spark = SparkSession.builder().appName("...").config("..., "...").getOrCreate()
 import spark.implicits._   // For implicit conversions
 ```

### Creating DataFrames

 - from an **existing RDD**
   - through reflection to infer the schema: **`RDD[People].toDF()`**
   - through a programmatic interface
     1. create an RDD of `Rows` from the original RDD : **`val rowRDD = ...`**
     2. create the schema represented by a `StructType` matching the structure of Rows in the RDD created in Step 1. : **`val schema = StructType(fields)`**
     3. apply the schema to the RDD of `Rows` via `createDataFrame` : **`val df = spark.createDataFrame(rowRDD, schema)`**
 - from a [**Hive table**](https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)
   - When working with Hive, one must **instantiate SparkSession with Hive support**.
   - `val spark = SparkSession.builder().appName("...").config("spark.sql.warehouse.dir", warehouseLocation).enableHiveSupport().getOrCreate()`
   - `val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")`
 - from **Spark data sources**
   - `spark.read.format("json").load("...")` or `spark.read.json("...")` for short   // `json`, `parquet`, `jdbc`, `orc`, `libsvm`, `csv`, `text`, (`parquet` as the default)

### Untyped Dataset Operations (aka DataFrame Operations)

 DataFrames provide a domain-specific language (DSL) for structured data manipulation.

 **Untyped transformations**

 [Dataset API Documentation](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)

 [DataFrame Function Reference](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)

### Running SQL Queries Programmatically

### Global Temporary View

### Creating Datasets

### Interoperating with RDDs




## Data Sources

### Generic Load/Save Functions

### Parquet Files

### JSON Datasets

### Hive Tables

### JDBC to Other Databases

### Troubleshooting




## Performance Tuning

### Caching Data in Memory

### Other Configuration Options




## Distributed SQL Engine

### Running the Thrift JDBC/ODBC server

### Running the Spark SQL CLI




## Reference

### Data Types

 All data types of Spark SQL are located in the package **`org.apache.spark.sql.types`**.

 Complex data types:
 - **`ArrayType(elementType, containsNull)`**
 - **`MapType(keyType, valueType, valueContainsNull)`**
 - **`StructType(fields)`**, where fileds are a sequence of `StructFields`.
   - **`StructFields(name, dataType, nullable)`**


### NaN Semantics

 - `NaN = NaN` return s **true**.
 - In **aggregations**, all NaN values are **grouped together**.
 - NaN is **treated as a normal value** in **join keys**.
 - NaN values is **larger** than any other numeric value as an **order**.
