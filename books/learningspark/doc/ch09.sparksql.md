# Spark SQL

 > As Spark SQL has been improved a lot in 2.1, lots of differences exist in the latest Spark SQL and the version used in the book, so the main content is based on the [official document](https://spark.apache.org/docs/2.1.0/sql-programming-guide.html).

## Overview

 Spark SQL uses the extra **structure of the data and the computation being performed information** to perform extra **optimizations**.

 There are several ways to interact with Spark SQL including **SQL** and **the Dataset API**.

### SQL

 - The results of running SQL **within another programming language** will be returned as a **`Dataset/DataFrame`**.
 - **Interact with the SQL** interface using the **commandline** or **over JDBC/ODBC**.

### Datasets and DataFrames

 A **Dataset&** is **a distributed collection** of data.
 - benefits of RDDs (**strong typing**, ability to use **powerful lambda functions**)
 - benefits of Spark SQL's **optimized execution engine**.

 A **DataFrame** is a **`Dataset[Row]`** **organized into named columns** which is **conceptually equivalent to a table** in a relational database or a data frame in R/Python.
 - **Sources**: structured data files, tables in Hive, external databases, or existing RDDs.


## Getting Started

### Starting Point: SparkSession

 ``` scala
 val spark = SparkSession.builder().appName("...").config("..., "...").getOrCreate()
 import spark.implicits._   // For implicit conversions
 ```

### Creating DataFrames

 - from an **existing RDD**
   - through reflection to infer the schema: **`RDD[People].toDF()`**
   - through a programmatic interface
     1. create an RDD of `Rows` from the original RDD : **`val rowRDD = ...`**
     2. create the schema represented by a `StructType` matching the structure of Rows in the RDD created in Step 1. : **`val schema = StructType(fields)`**
     3. apply the schema to the RDD of `Rows` via `createDataFrame` : **`val df = spark.createDataFrame(rowRDD, schema)`**
 - from a [**Hive table**](https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)
   - When working with Hive, one must **instantiate SparkSession with Hive support**.
   - `val spark = SparkSession.builder().appName("...").config("spark.sql.warehouse.dir", warehouseLocation).enableHiveSupport().getOrCreate()`
   - `val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")`
 - from **Spark data sources**
   - `spark.read.format("json").load("...")` or `spark.read.json("...")` for short   // `json`, `parquet`, `jdbc`, `orc`, `libsvm`, `csv`, `text`, (`parquet` as the default)

### Untyped Dataset Operations (aka DataFrame Operations)

 DataFrames provide a domain-specific language (DSL) for structured data manipulation.

 **Untyped transformations**

 [Dataset API Documentation](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)

 [DataFrame Function Reference](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$)

### Running SQL Queries Programmatically

 The **`sql`** function on a **`SparkSession`** enables applications to **run SQL queries programmatically** and **returns** the result as a **`DataFrame`**.

 ``` scala
 val df = spark.sql("SELECT * from people")
 ```

### Global Temporary View

 **Temporary views** are **session-scoped** and will **disappear if the session that creates it terminates**.

 **Global temporary view** is tied to a system preserved database **`global_temp`** and is **shared among all sessions** and **keep alive until Spark application terminates**.
 - `df.createGlobalTempView("people")`


### Creating Datasets

 Datasets use a specialized [**`Encoder`**](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder) to **serialize** the objects for processing or transmitting over the network.
 - `x.toDS()` where `x` is a common type including `Seq`, ...
 - `df.as[Person]` where `df` is a DataFrame



## Data Sources

 Spark SQL supports operating on a variety of data sources through the **DataFrame interface**.

### Generic Load/Save Functions

 `spark.read.load("...")`
 - `spark.read.format("json").load("...")`
 - `spark.read.json("...")`

 Or run SQL on files directly
 ``` scala
 spark.sql("SELECT * FROM parquet.`examples/src/.../users.parquet`")
 ```

 `df.write.save("...")`
 - `df.write.format("parquet").save("...")`
 - **`SaveMode`**: `ErrorIfExists`, `Append`, `Overwrite`, `Ignore`

 Or save to Hive metastore using the **`df.saveAsTable`** command.

### Parquet Files

 When **writing Parquet files**, all columns are automatically **converted to be nullable** for compatibility reasons.
 
 ``` scala
 val parquetFileDF = spark.read.parquet("people.parquet")
 parquetFileDF.createOrReplaceTempView("parquetFile")
 spark.sql("SELECT name FROM parquetFile WHERE ...")
 ```

 In a **partitioned table**, data are usually **stored in different directories**, with partitioning column values encoded in the path of each partition directory. Please refer to [Partition Discovery](https://spark.apache.org/docs/latest/sql-programming-guide.html#partition-discovery)

 Schema Merging


### JSON Datasets

### Hive Tables

 - from a [**Hive table**](https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables)
   - When working with Hive, one must **instantiate SparkSession with Hive support**.
   - `val spark = SparkSession.builder().appName("...").config("spark.sql.warehouse.dir", warehouseLocation).enableHiveSupport().getOrCreate()`
   - `val sqlDF = sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")`

### JDBC to Other Databases

 Tables from the remote database can be loaded as a DataFrame or Spark SQL temporary view using the Data Sources API.

### Troubleshooting




## Performance Tuning

 By either **caching data in memory**, or by **turning on some experimental options**.

### Caching Data in Memory

 Spark SQL can cache tables using an in-memory columnar format by calling **`spark.cacheTable("tableName")`** or **`dataFrame.cache()`**.

 **`SparkSession.setConf`** or **`SET key=value` commands using SQL** for configurating the in-memory caching.
 - `spark.sql.inMemoryColumnarStorage.compressed`
 - `spark.sql.inMemoryColumnarStorage.batchSize`

### Other Configuration Options
 - `spark.sql.files.maxPartitionBytes`
 - `spark.sql.files.openCostInBytes`
 - `spark.sql.broadcastTimeout`
 - `spark.sql.autoBroadcastJoinThreshold`




## Distributed SQL Engine

 Act **as a distributed query engine** using its **JDBC/ODBC** or **command-line** interface.

### Running the Thrift JDBC/ODBC server

 ``` shell
 export HIVE_SERVER2_THRIFT_PORT
 export HIVE_SERVER2_THRIFT_BIND_HOST
 ./sbin/start-thriftserver.sh
 ```

 Thrift JDBC server also supports sending thrift RPC messages over HTTP transport.
 - `hive.server2.transport.mode`
 - `hive.server2.thrift.http.port`
 - `hive.server2.http.endpoint`


### Running the Spark SQL CLI

 **`./bin/spark-sql`** to run the Hive metastore service in local mode and execute queries input from the command line.



## Reference

### Data Types

 All data types of Spark SQL are located in the package **`org.apache.spark.sql.types`**.

 Complex data types:
 - **`ArrayType(elementType, containsNull)`**
 - **`MapType(keyType, valueType, valueContainsNull)`**
 - **`StructType(fields)`**, where fileds are a sequence of `StructFields`.
   - **`StructFields(name, dataType, nullable)`**


### NaN Semantics

 - `NaN = NaN` return s **true**.
 - In **aggregations**, all NaN values are **grouped together**.
 - NaN is **treated as a normal value** in **join keys**.
 - NaN values is **larger** than any other numeric value as an **order**.
