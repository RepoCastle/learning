# Tuning and Debugging Spark

 - How to configure a Spark application
 - tune and debug production Spark workloads

## Configuring Spark with SparkConf

 - **`SparkConf`** instance: contains **key/value pairs of configuration options** the user would like **to override** with **`set()`**
 - through **`spark-submit`**: **`--conf`** flag accepts **any Spark configuration value**.
 - through **`spark-submit`**: **`--properties-file` CONFIG.conf** to read **whitespace-delimited** key/value pairs (default is: `conf/spark-defaults.conf`)

 Precedence order of configuration:
 **`set()` function** on as `SparkConf` object > **`spark-submit` flags** > values in **properties file** > **default values**
 
 Full Spark configuration options can be found in [the Spark documentation](http://spark.apache.org/docs/latest/configuration.html)
 
 **Almost all** Spark configurations occur **through the SparkConf** construct, **except `SPARK_LOCAL_DIRS`**
 - it set the **local storage directories** for Spark to use **for shuffle data**.
 - Because its value *may be* **different on different physical hosts**.



## Components of Execution: Jobs, Tasks, and Stages




## Finding Information

### Spark Web UI

### Driver and Executor Logs



## Key Performance Considerations

### Level of Parallelism

### Serialization Format

### Memory Management

### Hardware Provisioning

