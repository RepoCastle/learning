# Tuning and Debugging Spark

 - How to configure a Spark application
 - tune and debug production Spark workloads

## Configuring Spark with SparkConf

 - **`SparkConf`** instance: contains **key/value pairs of configuration options** the user would like **to override** with **`set()`**
 - through **`spark-submit`**: **`--conf`** flag accepts **any Spark configuration value**.
 - through **`spark-submit`**: **`--properties-file` CONFIG.conf** to read **whitespace-delimited** key/value pairs (default is: `conf/spark-defaults.conf`)

 Precedence order of configuration:
 - **`set()` function** on as `SparkConf` object > **`spark-submit` flags** > values in **properties file** > **default values**
 
 Full Spark configuration options can be found in [the Spark documentation](http://spark.apache.org/docs/latest/configuration.html)
 
 **Almost all** Spark configurations occur **through the SparkConf** construct, **except `SPARK_LOCAL_DIRS`**
 - it set the **local storage directories** for Spark to use **for shuffle data**.
 - Because its value *may be* **different on different physical hosts**.



## Components of Execution: Jobs, Tasks, and Stages

 - Spark's "logical" representation of RDDs and their partitions
 - **Merge multiple operations into tasks** to translate the logical representations into a physical execution plan

 Spark will form a directed acyclic graph (DAG) of RDD objects (each RDD maintains a **pointer to one or more parents** along **with metadata about what type of relationship** they have)

 **`toDebugString()`** will display the lineage of an RDD.

 Spark's scheduler **starts at the final RDD** being computed, and **works backward** to find what it must compute.

 **one task per partition**

 The scheduler will perform **pipelining, or collapsing** of **multiple RDDs into a single stage**.
 - **pipeline**: when RDDs can be computed from their parents without data movement
 - **truncate the lineage** of the RDD graph if existing RDD has already **been persisted** or **materialized by an earlier shuffle**.

 The **set of stages** *produced for a particular action* is termed **a job**.

 A physical stage will launch **tasks that each do the same thing but on specific partitions** of data.
 1. **Fetching its input**, either from data storage, and existing RDD, or shuffle outputs.
 2. **Performing the operation** necessary to compute RDD(s) that is represents.
 3. **Writing output** to a shuffle, to external storage, or back to the driver.

 Most logging and instrumentation in Spark is expressed in terms of stages, tasks, and shuffles.

 The following phases occur during Spark execution:
 1. User code defines a DAG of RDDs
 2. Actions force translation of the DAG to an execution plan
 3. Tasks are scheduled and executed on a cluster


## Finding Information

 - the Spark web UI
 - the logfiles produced by the driver and executor processes

### Spark Web UI

 On **the machine where the driver is** running, and the port is **4040** by default

 - **Jobs**: progress of running jobs, stages, and tasks (Details are in the **Stages** page, for example, to find task skew)
 - **Storage**: Information for RDDs that are persisted (An RDD is persisted if someone called **`persist()`** on the RDD **and it was later computed** in some job.)
 - **Executors**: A list of executors present in the application (**Stack trace from executors** unsing the **Thread Dump** button)
 - **Environment**: Debugging Spark's configuration


### Driver and Executor Logs

 Location of the logfiles:
 - **Standalone mode**: in `work/` directory of the Spark distribution on each worker
 - **Mesos**: in `work/` directory of a Mesos slave, and can be accessed from the Mesos master UI.
 - **YARN**:
   - `yarn logs -applicationId <app ID>` when application done
   - through ResourceManager UI to **the Nodes page** for a running application

 use `conf/log4j.properties` to control the logging behavior (or `--files` flag of `spark-submit`)



## Key Performance Considerations

  - code-level changes to improve performance (parallelism level, serialization format, memory management)
  - tuning the cluster and environment in which Spark runs

### Level of Parallelism

  **one partition** -> **a single task** -> runs on **a single core** in the cluster (by default)

  - Input RDDs typically choose parallelism based on the underlying storage system (**one partition for each block** of the HDFS file)
  - RDDs that are **derived from shuffling** other RDDs will have parallelism set **based on the size of their parent** RDDs

  How parallelism affect performance
  - If there is too little parallelism, Spark might leave resources idle.
  - If there is too much parallelism, small overheads associated with each partition can add up and become significant.

  Two ways to **tune the degree of parallelism** for operations
  - during operations that shuffle data, you can always **give a degree of parallelism** for the produced RDD **as a parameter**
  - redistribute existing RDD to have more or fewer partitions (**`repartition()`**, or more efficient with **`coalesce()`**)


### Serialization Format

  **`SparkConf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")`**

  When Spark is transferring data over the network or spilling data to disk, it needs to serialize objects into a binary format. (**Shuffle operations**)
  - Java's serialization
  - Kryo (better)

  **`NotSerializableException`** might occur if your code refers to a class that does **not extend Java's Serializable interface**.
  You may debug it with **`-Dsun.io.serialization.extended DebugInfo=true`** using **`--driver-java-options`** and **`--executor-java-options`**

### Memory Management

  Memory is used for a few **purpose**:
  - **RDD Storage**: `persist()` or `cache()`, cached in **JVM's heap**   (`spark.storage.memoryFraction`, Default: 60%)
  - **Shuffle and aggregation buffers**: intermediate buffers for storing shuffle output data (`spark.shuffle.memoryFraction`, Default: 20%)
  - **User code**: user application may allocate large arrays or other objects.

  Caching policy improvement:
  - `persist()` with **`MEMORY_AND_DISK`** might be better than `MEMORY_ONLY` **if the recomputation of RDD is expensive**.
  - **`MEMORY_AND_DISK_SER` or `MEMORY_ONLY_SER`** storage levels might be better **if you are cacheing large amounts of data** as objects.

  The **cost of garbage collection** scales with the **number of objects** on the heap, **not the number of bytes of data**.
  Caching serialized objects will take many objects and serialize them into a single giant buffer.


### Hardware Provisioning

  - amount of memory given to each executor: `spark.executor.memory` or `--executor-memory`
  - number of cores for each executor: `spark.executor.cores` or (`--executor-cores`)
  - total numbers of executors: `--num-executors`, (and `spark.cores.max` for Mesos and Standalone mode)
  - number of local disks to use for scratch data: used during shuffle operations, `SPARK_LOCAL_DIRS` environment variable

  Using **very large heap sizes** can cause garbage collection pauses to **hurt the throughput** of a Spark job. (64GB or less would be better)

  In Sparkâ€™s **Standalone mode**, you need to **launch multiple workers** (determined using **`SPARK_WORKER_INSTANCES`** ) **for a single application** to run **more than one executor on a host**.

